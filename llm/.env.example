# LLM Provider Selection: "ollama", "gemini", or "middleware"
# Use "ollama" for local inference (no rate limits, private)
# Use "gemini" for cloud inference (faster, but has rate limits)
# Use "middleware" for Open LLM Middleware with multi-provider support (RECOMMENDED)
LLM_PROVIDER=middleware

# ==================== Open LLM Middleware Configuration (RECOMMENDED) ====================
# Multi-provider LLM middleware with automatic failover, retry logic, and WebSocket support
# Supports: OpenAI, Google Gemini, Cerebras, Groq
# Live server: https://open-llm-mslh.onrender.com

LLM_MIDDLEWARE_URL=https://open-llm-mslh.onrender.com
LLM_MIDDLEWARE_SECRET=your_secret_code_here

# Middleware provider: "openai", "google", "cerebras", "groq"
# - openai: GPT-4o (best quality)
# - google: Gemini 2.5 Flash (default, 1M context window)
# - cerebras: Llama 3.3 70B (fastest inference, 30 req/min limit)
# - groq: Llama 3.3 70B Versatile (fast inference)
LLM_MIDDLEWARE_PROVIDER=google

# Optional: Specify model (leave empty for provider default)
# OpenAI: gpt-4o, gpt-4o-mini, gpt-3.5-turbo
# Google: gemini-2.5-flash, gemini-1.5-pro
# Cerebras: llama-3.3-70b
# Groq: llama-3.3-70b-versatile
LLM_MIDDLEWARE_MODEL=

# Request timeout in seconds (increase for complex queries)
LLM_MIDDLEWARE_TIMEOUT=120

# Maximum tokens in response
LLM_MIDDLEWARE_MAX_TOKENS=2048

# Use WebSocket for faster bidirectional communication (experimental)
LLM_MIDDLEWARE_USE_WEBSOCKET=false

# ==================== Ollama Configuration (Local LLM) ====================
# Make sure Ollama is running: ollama serve
# Pull a fast model first: ollama pull llama3.2

OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2

# Available Ollama models (sorted by speed):
# - llama3.2 (RECOMMENDED - fast, 3B params, 20-40s response)
# - qwen2.5:3b (very fast, good quality)
# - phi3.5 (fast, Microsoft model)
# - phi4-mini (slower but better reasoning, 3.8B params)
# - mistral (slower, 7B params)

# Use Gemini for web search results (faster for complex web content)
USE_GEMINI_FOR_WEB_SEARCH=true

# Gemini API Configuration (Cloud LLM - has rate limits)
# Get your API key from https://makersuite.google.com/app/apikey
GEMINI_API_KEY=your_gemini_api_key_here
GEMINI_MODEL=gemini-1.5-flash

# Available Gemini models:
# - gemini-1.5-flash (fast, cost-effective)
# - gemini-1.5-pro (more capable, higher quality)
# - gemini-2.0-flash-exp (latest experimental)

# Tavily API Configuration (for web search when data not in local DB)
# Get your API key from https://tavily.com/
TAVILY_API_KEY=your_tavily_api_key_here
