# LLM Provider Selection: "ollama" or "gemini"
# Use "ollama" for local inference (no rate limits, private)
# Use "gemini" for cloud inference (faster, but has rate limits)
LLM_PROVIDER=ollama

# Ollama Configuration (Local LLM - no rate limits!)
# Make sure Ollama is running: ollama serve
# Pull a fast model first: ollama pull llama3.2

OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2

# Available Ollama models (sorted by speed):
# - llama3.2 (RECOMMENDED - fast, 3B params, 20-40s response)
# - qwen2.5:3b (very fast, good quality)
# - phi3.5 (fast, Microsoft model)
# - phi4-mini (slower but better reasoning, 3.8B params)
# - mistral (slower, 7B params)

# Use Gemini for web search results (faster for complex web content)
USE_GEMINI_FOR_WEB_SEARCH=true

# Gemini API Configuration (Cloud LLM - has rate limits)
# Get your API key from https://makersuite.google.com/app/apikey
GEMINI_API_KEY=your_gemini_api_key_here
GEMINI_MODEL=gemini-1.5-flash

# Available Gemini models:
# - gemini-1.5-flash (fast, cost-effective)
# - gemini-1.5-pro (more capable, higher quality)
# - gemini-2.0-flash-exp (latest experimental)

# Tavily API Configuration (for web search when data not in local DB)
# Get your API key from https://tavily.com/
TAVILY_API_KEY=your_tavily_api_key_here
